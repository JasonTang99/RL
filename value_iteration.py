# -*- coding: utf-8 -*-
"""value_iteration

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBKHjvNY60F7waLa-fkqI-1-9pe0PGns
"""

# This code is based on an exercise in the Reinforcement Learning textbook by Richard Sutton and Andrew Barto
# Link to the textbook: http://incompleteideas.net/book/RLbook2018.pdf
# The exercise is the car rental exercise found in section 4.4 on page 84
# The dependencies for this code are numpy and python 3.6+

import numpy as np

NUM_STATES = 101 # 0 - 100
V_s = np.zeros(NUM_STATES)
V_s[NUM_STATES - 1] = 1 # Only place where reward is 1

V_index = np.array(range(1, 100))

policy = np.ones(99) # 1 - 99 are not terminal states

def get_actions(s):
  return list(range(1, min(s, 100 - s) + 1))

DELTA_LIM = 0.000001
DISCOUNT = 1

p_h = 0.4

# At most there are 50 actions to take (at state 50, actions: [1,50])

prob_table = []

for s in V_index: # [1, 99]
  heads = []
  tails = []
  for a in get_actions(s): # [1, min(s, 100 - s)]
    heads.append([p_h, 0, s + a])
    tails.append([1 - p_h, 0, s - a])

  for _ in range(50 - len(heads)):
    heads.append([0,0,0])
    tails.append([0,0,0])

  prob_table.append(np.stack([np.array(heads), np.array(tails)], axis = 1))
  
prob_table = np.array(prob_table)
prob_table.shape

def value_iteration_optimized(V_s):
  delta = DELTA_LIM + 1
  
  new_policy = None
  
  while delta > DELTA_LIM:
    delta = 0
  
    v = V_s.copy()
    
    reward = prob_table[:, :, :, 1] + DISCOUNT * v[prob_table[:, :, :, 2].astype(np.intp)] # (99, 50, 2)
    reward *= prob_table[:, :, :, 0] # (99, 50, 2)
    reward = np.sum(reward, axis=2) # (99, 50)
    
    V_s = np.max(reward, axis = 1) # (99)
    V_s = np.array([0] + V_s.tolist() + [1])
    
    new_policy = np.argmax(reward, axis = 1) + 1
    
    delta = np.amax(np.abs(v - V_s))
    
    print("DELTA", np.round(delta, 6))
    
  return new_policy, V_s



def P(s, a):
  return [[p_h, s + a], [1-p_h, s - a]]

def value_iteration(V_s):
  delta = DELTA_LIM + 1
  
  new_policy = np.ones(99)
  
  while delta > DELTA_LIM:
    delta = 0
    for s in range(1, NUM_STATES - 1):
      v = V_s[s]
      rewards = []
      for a in get_actions(s):
        reward = 0
        for prob, next_state in P(s, a):
          if next_state == 101:
            reward += prob * (1 + DISCOUNT * V_s[next_state])
          else:
            reward += prob * (DISCOUNT * V_s[next_state])
        rewards.append(reward)
      V_s[s] = max(rewards)
      new_policy[s - 1] = np.argmax(rewards) + 1
      delta = max(delta, abs(v - V_s[s]))
    print("DELTA", np.round(delta, 6))
    
  return new_policy, V_s

policy, V_s = value_iteration_optimized(V_s)
print(policy)
print(V_s)

import matplotlib.pyplot as plt
plt.bar(range(1,100), policy)
plt.show()

plt.plot(V_s)
plt.show()